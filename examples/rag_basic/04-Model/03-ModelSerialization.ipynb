{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2443ac2d",
   "metadata": {},
   "source": [
    "# 03. LangChain 모델 직렬화(Serialization) 가이드\n",
    "\n",
    "이 노트북에서는 LangChain 모델과 체인을 저장하고 불러오는 방법을 배웁니다.\n",
    "\n",
    "## 목차\n",
    "1. 환경 설정 및 LangSmith 연결\n",
    "2. 직렬화 가능성 확인 (`is_lc_serializable`)\n",
    "3. JSON 직렬화 (`dumps`, `dumpd`)\n",
    "4. Pickle 직렬화\n",
    "5. 모델 저장 및 불러오기 실습\n",
    "6. 체인 직렬화 실습\n",
    "\n",
    "## 개요\n",
    "\n",
    "모델 직렬화는 다음과 같은 상황에서 유용합니다:\n",
    "- **모델 배포**: 훈련된 모델을 프로덕션 환경에 배포\n",
    "- **캐싱**: 복잡한 체인 구성을 저장하여 재사용\n",
    "- **버전 관리**: 모델의 다양한 버전을 관리\n",
    "- **공유**: 팀원 간 모델 설정 공유\n",
    "\n",
    "## 직렬화 방법 비교\n",
    "\n",
    "| 방법 | 장점 | 단점 | 용도 |\n",
    "|------|------|------|------|\n",
    "| JSON (`dumps`) | 가독성 좋음, 플랫폼 독립적 | 일부 객체 직렬화 불가 | 설정 공유, 디버깅 |\n",
    "| Pickle | 모든 Python 객체 지원 | Python 전용, 보안 위험 | 완전한 객체 저장 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ca3e3",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 LangSmith 연결\n",
    "\n",
    "먼저 필요한 패키지를 임포트하고 환경 변수를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9afb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.load import dumps, dumpd, loads\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# is_lc_serializable 함수 임포트 (최신 버전 대응)\n",
    "try:\n",
    "    from langchain_core.load import dumpd as _test_dumpd\n",
    "    def is_lc_serializable(obj):\n",
    "        \"\"\"객체가 LangChain 직렬화 가능한지 확인하는 함수\"\"\"\n",
    "        try:\n",
    "            _test_dumpd(obj)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "except ImportError:\n",
    "    # 대체 구현\n",
    "    def is_lc_serializable(obj):\n",
    "        \"\"\"기본 구현: dumpd로 테스트\"\"\"\n",
    "        try:\n",
    "            from langchain_core.load import dumpd\n",
    "            dumpd(obj)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수 확인\n",
    "print(\"🔍 환경 변수 확인 중...\")\n",
    "\n",
    "# Google API 키 확인\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"❌ GOOGLE_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "print(\"✅ Google API Key: 설정됨\")\n",
    "\n",
    "# LangSmith API 키 확인 (선택사항)\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "if langsmith_api_key:\n",
    "    print(\"✅ LangSmith API Key: 설정됨\")\n",
    "    # LangSmith 설정\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # LangSmith 추적 활성화\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"model-serialization-demo\"  # 프로젝트 이름 설정\n",
    "    \n",
    "    print(f\"📊 LangSmith 프로젝트: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "    print(f\"🔄 LangSmith 추적 활성화: {os.environ['LANGCHAIN_TRACING_V2']}\")\n",
    "    print(\"🌐 LangSmith 대시보드: https://smith.langchain.com/\")\n",
    "else:\n",
    "    print(\"⚠️  LangSmith API Key가 설정되지 않았습니다.\")\n",
    "    print(\"   LangSmith 추적 없이 계속 진행합니다.\")\n",
    "\n",
    "print(\"\\n✅ 환경 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 모델 초기화\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=200\n",
    ")\n",
    "\n",
    "print(\"🤖 Gemini 모델 초기화 완료!\")\n",
    "print(f\"   모델: {llm.model}\")\n",
    "print(f\"   온도: {llm.temperature}\")\n",
    "print(f\"   최대 토큰: {llm.max_output_tokens}\")\n",
    "\n",
    "# 간단한 테스트\n",
    "test_response = llm.invoke(\"안녕하세요!\")\n",
    "print(f\"\\n🧪 테스트 응답: {test_response.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637059e0",
   "metadata": {},
   "source": [
    "## 2. 직렬화 가능성 확인 (`is_lc_serializable`)\n",
    "\n",
    "LangChain의 `is_lc_serializable` 함수를 사용하여 객체가 직렬화 가능한지 확인할 수 있습니다.\n",
    "\n",
    "### 직렬화란?\n",
    "- **직렬화(Serialization)**: 객체를 저장이나 전송 가능한 형태로 변환\n",
    "- **역직렬화(Deserialization)**: 저장된 데이터를 다시 객체로 복원\n",
    "\n",
    "### LangChain 직렬화 지원\n",
    "- LangChain 컴포넌트들은 대부분 직렬화를 지원\n",
    "- `@serializable` 데코레이터로 직렬화 가능 클래스 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 다양한 객체의 직렬화 가능성 확인\n",
    "############################################################################\n",
    "\n",
    "print(\"🔍 직렬화 가능성 확인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. LLM 모델 확인\n",
    "print(f\"1. ChatGoogleGenerativeAI: {is_lc_serializable(llm)}\")\n",
    "\n",
    "# 2. 프롬프트 템플릿 확인\n",
    "prompt = ChatPromptTemplate.from_template(\"다음 질문에 답해주세요: {question}\")\n",
    "print(f\"2. ChatPromptTemplate: {is_lc_serializable(prompt)}\")\n",
    "\n",
    "# 3. 출력 파서 확인\n",
    "parser = StrOutputParser()\n",
    "print(f\"3. StrOutputParser: {is_lc_serializable(parser)}\")\n",
    "\n",
    "# 4. 체인 확인 (LangChain Expression Language - LCEL)\n",
    "chain = prompt | llm | parser\n",
    "print(f\"4. Chain (prompt | llm | parser): {is_lc_serializable(chain)}\")\n",
    "\n",
    "# 5. 개별 메시지 확인\n",
    "human_msg = HumanMessage(content=\"안녕하세요\")\n",
    "system_msg = SystemMessage(content=\"당신은 도움이 되는 AI입니다\")\n",
    "print(f\"5. HumanMessage: {is_lc_serializable(human_msg)}\")\n",
    "print(f\"6. SystemMessage: {is_lc_serializable(system_msg)}\")\n",
    "\n",
    "# 6. 일반 Python 객체 확인\n",
    "regular_dict = {\"key\": \"value\"}\n",
    "print(f\"7. Regular Dict: {is_lc_serializable(regular_dict)}\")\n",
    "\n",
    "print(\"\\n✅ 직렬화 가능성 확인 완료!\")\n",
    "print(\"📝 True: LangChain 직렬화 지원, False: 지원하지 않음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818530c",
   "metadata": {},
   "source": [
    "## 3. JSON 직렬화 (`dumps`, `dumpd`)\n",
    "\n",
    "LangChain에서 제공하는 JSON 직렬화 함수들을 사용해봅시다.\n",
    "\n",
    "### 함수별 특징\n",
    "- **`dumps()`**: 객체를 JSON 문자열로 직렬화\n",
    "- **`dumpd()`**: 객체를 Python 딕셔너리로 직렬화\n",
    "- **`loads()`**: 직렬화된 데이터를 다시 객체로 역직렬화\n",
    "\n",
    "### 장점\n",
    "- **가독성**: JSON 형태로 사람이 읽기 쉬움\n",
    "- **호환성**: 다양한 언어와 플랫폼에서 지원\n",
    "- **디버깅**: 구조를 쉽게 파악 가능\n",
    "\n",
    "### 단점\n",
    "- **제한성**: LangChain 지원 객체만 직렬화 가능\n",
    "- **크기**: 바이너리 형식보다 용량이 큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab213c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# JSON 직렬화 실습\n",
    "############################################################################\n",
    "\n",
    "print(\"📦 JSON 직렬화 실습\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 프롬프트 템플릿 직렬화\n",
    "print(\"1. 프롬프트 템플릿 직렬화\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# dumps() - JSON 문자열로 변환\n",
    "prompt_json_str = dumps(prompt)\n",
    "print(f\"✅ dumps() 성공! 길이: {len(prompt_json_str)} 문자\")\n",
    "print(f\"📄 JSON 미리보기: {prompt_json_str[:100]}...\")\n",
    "\n",
    "# dumpd() - Python 딕셔너리로 변환\n",
    "prompt_dict = dumpd(prompt)\n",
    "print(f\"\\n✅ dumpd() 성공! 키 개수: {len(prompt_dict)} 개\")\n",
    "print(f\"🔑 딕셔너리 키들: {list(prompt_dict.keys())}\")\n",
    "\n",
    "# 2. 체인 직렬화\n",
    "print(\"\\n\\n2. 체인 직렬화\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    chain_json_str = dumps(chain)\n",
    "    print(f\"✅ 체인 dumps() 성공! 길이: {len(chain_json_str)} 문자\")\n",
    "    \n",
    "    chain_dict = dumpd(chain)\n",
    "    print(f\"✅ 체인 dumpd() 성공! 키 개수: {len(chain_dict)} 개\")\n",
    "    print(f\"🔑 체인 딕셔너리 키들: {list(chain_dict.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 체인 직렬화 실패: {e}\")\n",
    "\n",
    "# 3. 개별 컴포넌트 확인\n",
    "print(\"\\n\\n3. 개별 컴포넌트 직렬화\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "components = {\n",
    "    \"프롬프트\": prompt,\n",
    "    \"파서\": parser\n",
    "}\n",
    "\n",
    "for name, component in components.items():\n",
    "    try:\n",
    "        json_str = dumps(component)\n",
    "        print(f\"✅ {name}: 성공 ({len(json_str)} 문자)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: 실패 - {e}\")\n",
    "\n",
    "print(\"\\n✅ JSON 직렬화 실습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# JSON 역직렬화 실습\n",
    "############################################################################\n",
    "\n",
    "print(\"🔄 JSON 역직렬화 실습\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 프롬프트 템플릿 복원\n",
    "print(\"1. 프롬프트 템플릿 복원\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# JSON 문자열에서 복원\n",
    "restored_prompt = loads(prompt_json_str)\n",
    "print(f\"✅ 프롬프트 복원 성공!\")\n",
    "print(f\"📝 원본 타입: {type(prompt)}\")\n",
    "print(f\"📝 복원 타입: {type(restored_prompt)}\")\n",
    "print(f\"🔍 동일성 확인: {type(prompt) == type(restored_prompt)}\")\n",
    "\n",
    "# 2. 복원된 프롬프트 테스트\n",
    "print(\"\\n2. 복원된 프롬프트 테스트\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_question = \"LangChain이란 무엇인가요?\"\n",
    "\n",
    "# 원본 프롬프트\n",
    "original_messages = prompt.format_messages(question=test_question)\n",
    "print(f\"📤 원본 프롬프트: {original_messages[0].content}\")\n",
    "\n",
    "# 복원된 프롬프트\n",
    "restored_messages = restored_prompt.format_messages(question=test_question)\n",
    "print(f\"📥 복원 프롬프트: {restored_messages[0].content}\")\n",
    "\n",
    "# 동일성 확인\n",
    "print(f\"🔍 메시지 동일성: {original_messages[0].content == restored_messages[0].content}\")\n",
    "\n",
    "# 3. 실제 LLM 호출 테스트\n",
    "print(\"\\n3. 복원된 프롬프트로 LLM 호출\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "response = llm.invoke(restored_messages)\n",
    "print(f\"💬 응답: {response.content[:100]}...\")\n",
    "\n",
    "print(\"\\n✅ JSON 역직렬화 실습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb4121",
   "metadata": {},
   "source": [
    "## 4. Pickle 직렬화\n",
    "\n",
    "Pickle은 Python의 표준 직렬화 라이브러리로, 거의 모든 Python 객체를 직렬화할 수 있습니다.\n",
    "\n",
    "### Pickle의 특징\n",
    "\n",
    "**장점:**\n",
    "- **완전성**: 거의 모든 Python 객체 지원\n",
    "- **효율성**: 바이너리 형식으로 크기가 작음\n",
    "- **속도**: JSON보다 빠른 직렬화/역직렬화\n",
    "\n",
    "**단점:**\n",
    "- **보안 위험**: 악의적인 코드 실행 가능\n",
    "- **Python 전용**: 다른 언어에서 읽을 수 없음\n",
    "- **버전 의존성**: Python 버전별 호환성 문제 가능\n",
    "\n",
    "### 사용 사례\n",
    "- **개발 환경**: 복잡한 객체의 임시 저장\n",
    "- **캐싱**: 연산 결과의 빠른 저장/로드\n",
    "- **모델 백업**: 전체 모델 상태 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Pickle 직렬화 실습\n",
    "############################################################################\n",
    "\n",
    "print(\"🥒 Pickle 직렬화 실습\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 단일 객체 Pickle 저장\n",
    "print(\"1. 단일 객체 Pickle 저장\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 프롬프트 템플릿을 pickle로 저장\n",
    "prompt_file = \"prompt_template.pkl\"\n",
    "with open(prompt_file, 'wb') as f:\n",
    "    pickle.dump(prompt, f)\n",
    "\n",
    "file_size = os.path.getsize(prompt_file)\n",
    "print(f\"✅ 프롬프트 템플릿 저장 완료!\")\n",
    "print(f\"📁 파일명: {prompt_file}\")\n",
    "print(f\"📏 파일 크기: {file_size} bytes\")\n",
    "\n",
    "# 2. 복합 객체 Pickle 저장\n",
    "print(\"\\n2. 복합 객체 Pickle 저장\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 여러 객체를 딕셔너리로 묶어서 저장 (LLM 제외)\n",
    "# LLM 객체는 gRPC 연결 때문에 pickle로 직렬화할 수 없음\n",
    "model_components_safe = {\n",
    "    \"prompt\": prompt,\n",
    "    \"parser\": parser,\n",
    "    \"chain_config\": dumpd(chain),  # 체인을 JSON 형태로 저장\n",
    "    \"llm_config\": {\n",
    "        \"model\": llm.model,\n",
    "        \"temperature\": llm.temperature,\n",
    "        \"max_output_tokens\": llm.max_output_tokens\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"created_at\": \"2025-09-05\",\n",
    "        \"model_name\": \"gemini-1.5-flash\",\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 저장 가능한 컴포넌트 확인:\")\n",
    "for name, component in model_components_safe.items():\n",
    "    if name in [\"metadata\", \"llm_config\"]:\n",
    "        print(f\"   ✅ {name}: 일반 딕셔너리 (저장 가능)\")\n",
    "    elif name == \"chain_config\":\n",
    "        print(f\"   ✅ {name}: JSON 직렬화된 체인 (저장 가능)\")\n",
    "    else:\n",
    "        try:\n",
    "            # 테스트로 pickle 직렬화 시도\n",
    "            pickle.dumps(component)\n",
    "            print(f\"   ✅ {name}: Pickle 직렬화 가능\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {name}: Pickle 직렬화 불가 - {str(e)[:50]}...\")\n",
    "\n",
    "components_file = \"model_components.pkl\"\n",
    "with open(components_file, 'wb') as f:\n",
    "    pickle.dump(model_components_safe, f)\n",
    "\n",
    "components_size = os.path.getsize(components_file)\n",
    "print(f\"\\n✅ 모델 컴포넌트 저장 완료!\")\n",
    "print(f\"📁 파일명: {components_file}\")\n",
    "print(f\"📏 파일 크기: {components_size} bytes\")\n",
    "print(f\"🔧 저장된 컴포넌트: {list(model_components_safe.keys())}\")\n",
    "\n",
    "print(\"\\n⚠️  참고: LLM 객체는 연결 정보 때문에 pickle로 저장할 수 없어\")\n",
    "print(\"   설정 정보만 저장하고 나중에 새로 생성해야 합니다.\")\n",
    "\n",
    "print(\"\\n✅ Pickle 저장 실습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Pickle 역직렬화 실습\n",
    "############################################################################\n",
    "\n",
    "print(\"🔄 Pickle 역직렬화 실습\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 단일 객체 불러오기\n",
    "print(\"1. 단일 객체 불러오기\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 프롬프트 템플릿 불러오기\n",
    "with open(prompt_file, 'rb') as f:\n",
    "    loaded_prompt = pickle.load(f)\n",
    "\n",
    "print(f\"✅ 프롬프트 템플릿 로드 완료!\")\n",
    "print(f\"📝 원본 타입: {type(prompt)}\")\n",
    "print(f\"📝 로드 타입: {type(loaded_prompt)}\")\n",
    "print(f\"🔍 타입 동일성: {type(prompt) == type(loaded_prompt)}\")\n",
    "\n",
    "# 2. 복합 객체 불러오기\n",
    "print(\"\\n2. 복합 객체 불러오기\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "with open(components_file, 'rb') as f:\n",
    "    loaded_components = pickle.load(f)\n",
    "\n",
    "print(f\"✅ 모델 컴포넌트 로드 완료!\")\n",
    "print(f\"🔧 로드된 컴포넌트: {list(loaded_components.keys())}\")\n",
    "print(f\"📊 메타데이터: {loaded_components['metadata']}\")\n",
    "\n",
    "# 3. 로드된 객체들 테스트\n",
    "print(\"\\n3. 로드된 객체들 테스트\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 로드된 프롬프트 테스트\n",
    "test_question = \"Python에서 Pickle의 장단점은?\"\n",
    "loaded_messages = loaded_prompt.format_messages(question=test_question)\n",
    "print(f\"📤 로드된 프롬프트: {loaded_messages[0].content}\")\n",
    "\n",
    "# 로드된 LLM으로 응답 생성\n",
    "if 'llm_config' in loaded_components:\n",
    "    llm_config = loaded_components['llm_config']\n",
    "    print(f\"🔧 LLM 설정 로드: {llm_config}\")\n",
    "    \n",
    "    # 새로운 LLM 객체 생성\n",
    "    recreated_llm = ChatGoogleGenerativeAI(\n",
    "        model=llm_config['model'],\n",
    "        temperature=llm_config['temperature'],\n",
    "        max_output_tokens=llm_config['max_output_tokens']\n",
    "    )\n",
    "    \n",
    "    response = recreated_llm.invoke(loaded_messages)\n",
    "    print(f\"💬 재생성된 LLM 응답: {response.content[:100]}...\")\n",
    "else:\n",
    "    print(\"⚠️  LLM 설정을 찾을 수 없습니다. 기존 LLM 사용...\")\n",
    "    response = llm.invoke(loaded_messages)\n",
    "    print(f\"💬 기존 LLM 응답: {response.content[:100]}...\")\n",
    "\n",
    "# 로드된 체인 설정에서 체인 복원\n",
    "if 'chain_config' in loaded_components:\n",
    "    try:\n",
    "        # JSON 직렬화된 체인 설정을 복원\n",
    "        chain_config = loaded_components['chain_config']\n",
    "        restored_chain = loads(json.dumps(chain_config))\n",
    "        \n",
    "        chain_response = restored_chain.invoke({\"question\": test_question})\n",
    "        print(f\"🔗 복원된 체인 응답: {chain_response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  체인 복원 에러: {e}\")\n",
    "        print(\"   기존 체인으로 테스트...\")\n",
    "        chain_response = chain.invoke({\"question\": test_question})\n",
    "        print(f\"🔗 기존 체인 응답: {chain_response[:100]}...\")\n",
    "else:\n",
    "    print(\"⚠️  체인 설정을 찾을 수 없습니다.\")\n",
    "\n",
    "print(\"\\n✅ Pickle 역직렬화 실습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988c38e",
   "metadata": {},
   "source": [
    "## 5. 모델 저장 및 불러오기 실전 예제\n",
    "\n",
    "실제 애플리케이션에서 사용할 수 있는 모델 저장/불러오기 패턴을 구현해봅시다.\n",
    "\n",
    "### 실전 시나리오\n",
    "1. **설정 저장**: 모델 파라미터와 프롬프트를 JSON으로 저장\n",
    "2. **모델 백업**: 전체 모델을 Pickle로 백업\n",
    "3. **버전 관리**: 다양한 버전의 모델 관리\n",
    "4. **안전한 로딩**: 에러 처리와 검증 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c79e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 모델 관리 클래스 구현\n",
    "############################################################################\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"LangChain 모델의 저장과 불러오기를 관리하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"saved_models\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_model_config(self, model_name, components):\n",
    "        \"\"\"JSON 형태로 모델 설정 저장\"\"\"\n",
    "        config_data = {\n",
    "            \"metadata\": {\n",
    "                \"name\": model_name,\n",
    "                \"created_at\": datetime.datetime.now().isoformat(),\n",
    "                \"langchain_version\": \"latest\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 직렬화 가능한 컴포넌트들만 저장\n",
    "        for name, component in components.items():\n",
    "            if is_lc_serializable(component):\n",
    "                try:\n",
    "                    config_data[name] = dumpd(component)\n",
    "                    print(f\"✅ {name}: JSON 저장 성공\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {name}: JSON 저장 실패 - {e}\")\n",
    "            else:\n",
    "                print(f\"⚠️  {name}: 직렬화 불가능 (JSON 저장 제외)\")\n",
    "        \n",
    "        # JSON 파일 저장\n",
    "        config_file = self.base_dir / f\"{model_name}_config.json\"\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"📁 설정 파일 저장: {config_file}\")\n",
    "        return config_file\n",
    "    \n",
    "    def save_model_pickle(self, model_name, components):\n",
    "        \"\"\"Pickle 형태로 전체 모델 저장\"\"\"\n",
    "        # LLM 객체는 pickle로 저장할 수 없으므로 안전한 형태로 변환\n",
    "        safe_components = {}\n",
    "        \n",
    "        for name, component in components.items():\n",
    "            if name == \"llm\":\n",
    "                # LLM 객체는 설정 정보만 저장\n",
    "                safe_components[\"llm_config\"] = {\n",
    "                    \"model\": component.model,\n",
    "                    \"temperature\": component.temperature,\n",
    "                    \"max_output_tokens\": component.max_output_tokens\n",
    "                }\n",
    "                print(f\"⚠️  {name}: LLM 객체를 설정 정보로 변환하여 저장\")\n",
    "            elif name == \"chain\":\n",
    "                # 체인은 JSON 직렬화하여 저장\n",
    "                try:\n",
    "                    safe_components[\"chain_config\"] = dumpd(component)\n",
    "                    print(f\"✅ {name}: 체인을 JSON 설정으로 변환하여 저장\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {name}: 체인 직렬화 실패 - {e}\")\n",
    "            else:\n",
    "                # 다른 컴포넌트들은 pickle 가능성 확인 후 저장\n",
    "                try:\n",
    "                    pickle.dumps(component)  # 테스트\n",
    "                    safe_components[name] = component\n",
    "                    print(f\"✅ {name}: Pickle 직렬화 가능\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {name}: Pickle 직렬화 불가 - {str(e)[:50]}...\")\n",
    "        \n",
    "        pickle_data = {\n",
    "            \"metadata\": {\n",
    "                \"name\": model_name,\n",
    "                \"created_at\": datetime.datetime.now().isoformat(),\n",
    "                \"python_version\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}\"\n",
    "            },\n",
    "            \"components\": safe_components\n",
    "        }\n",
    "        \n",
    "        pickle_file = self.base_dir / f\"{model_name}.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(pickle_data, f)\n",
    "        \n",
    "        print(f\"📁 Pickle 파일 저장: {pickle_file}\")\n",
    "        return pickle_file\n",
    "    \n",
    "    def load_model_config(self, model_name):\n",
    "        \"\"\"JSON 설정에서 모델 불러오기\"\"\"\n",
    "        config_file = self.base_dir / f\"{model_name}_config.json\"\n",
    "        \n",
    "        if not config_file.exists():\n",
    "            raise FileNotFoundError(f\"설정 파일을 찾을 수 없습니다: {config_file}\")\n",
    "        \n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            config_data = json.load(f)\n",
    "        \n",
    "        print(f\"📖 설정 파일 로드: {config_file}\")\n",
    "        print(f\"📊 메타데이터: {config_data['metadata']}\")\n",
    "        \n",
    "        # 컴포넌트 복원\n",
    "        components = {}\n",
    "        for name, component_data in config_data.items():\n",
    "            if name != \"metadata\":\n",
    "                try:\n",
    "                    components[name] = loads(json.dumps(component_data))\n",
    "                    print(f\"✅ {name}: JSON 복원 성공\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {name}: JSON 복원 실패 - {e}\")\n",
    "        \n",
    "        return components, config_data['metadata']\n",
    "    \n",
    "    def load_model_pickle(self, model_name):\n",
    "        \"\"\"Pickle 파일에서 모델 불러오기\"\"\"\n",
    "        pickle_file = self.base_dir / f\"{model_name}.pkl\"\n",
    "        \n",
    "        if not pickle_file.exists():\n",
    "            raise FileNotFoundError(f\"Pickle 파일을 찾을 수 없습니다: {pickle_file}\")\n",
    "        \n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"📖 Pickle 파일 로드: {pickle_file}\")\n",
    "        print(f\"📊 메타데이터: {pickle_data['metadata']}\")\n",
    "        \n",
    "        components = pickle_data['components']\n",
    "        \n",
    "        # LLM 설정이 있다면 새로운 LLM 객체 생성\n",
    "        if 'llm_config' in components:\n",
    "            try:\n",
    "                llm_config = components['llm_config']\n",
    "                components['llm'] = ChatGoogleGenerativeAI(\n",
    "                    model=llm_config['model'],\n",
    "                    temperature=llm_config['temperature'],\n",
    "                    max_output_tokens=llm_config['max_output_tokens']\n",
    "                )\n",
    "                print(\"✅ LLM 객체 재생성 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ LLM 객체 재생성 실패: {e}\")\n",
    "        \n",
    "        # 체인 설정이 있다면 체인 복원\n",
    "        if 'chain_config' in components:\n",
    "            try:\n",
    "                chain_config = components['chain_config']\n",
    "                components['chain'] = loads(json.dumps(chain_config))\n",
    "                print(\"✅ 체인 객체 복원 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 체인 객체 복원 실패: {e}\")\n",
    "        \n",
    "        return components, pickle_data['metadata']\n",
    "    \n",
    "    def list_saved_models(self):\n",
    "        \"\"\"저장된 모델 목록 반환\"\"\"\n",
    "        models = []\n",
    "        for file in self.base_dir.glob(\"*\"):\n",
    "            if file.suffix == \".json\" and \"_config\" in file.name:\n",
    "                model_name = file.name.replace(\"_config.json\", \"\")\n",
    "                models.append({\n",
    "                    \"name\": model_name,\n",
    "                    \"config_file\": file,\n",
    "                    \"pickle_file\": self.base_dir / f\"{model_name}.pkl\"\n",
    "                })\n",
    "        return models\n",
    "\n",
    "# ModelManager 인스턴스 생성\n",
    "model_manager = ModelManager()\n",
    "print(\"🎯 ModelManager 초기화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d09aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 실전 예제 실행\n",
    "############################################################################\n",
    "\n",
    "print(\"🚀 실전 모델 저장/불러오기 예제\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 모델 저장\n",
    "print(\"1. 모델 저장하기\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 저장할 컴포넌트들 준비\n",
    "my_components = {\n",
    "    \"llm\": llm,\n",
    "    \"prompt\": prompt,\n",
    "    \"parser\": parser,\n",
    "    \"chain\": chain\n",
    "}\n",
    "\n",
    "model_name = \"gemini_qa_model_v1\"\n",
    "\n",
    "# JSON 설정 저장\n",
    "print(\"🔧 JSON 설정 저장 중...\")\n",
    "config_file = model_manager.save_model_config(model_name, my_components)\n",
    "\n",
    "# Pickle 전체 저장\n",
    "print(\"\\n🥒 Pickle 전체 저장 중...\")\n",
    "pickle_file = model_manager.save_model_pickle(model_name, my_components)\n",
    "\n",
    "# 2. 저장된 모델 목록 확인\n",
    "print(\"\\n\\n2. 저장된 모델 목록\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "saved_models = model_manager.list_saved_models()\n",
    "for model_info in saved_models:\n",
    "    config_exists = model_info[\"config_file\"].exists()\n",
    "    pickle_exists = model_info[\"pickle_file\"].exists()\n",
    "    \n",
    "    print(f\"📋 모델명: {model_info['name']}\")\n",
    "    print(f\"   ✅ 설정 파일: {'있음' if config_exists else '없음'}\")\n",
    "    print(f\"   ✅ Pickle 파일: {'있음' if pickle_exists else '없음'}\")\n",
    "\n",
    "print(\"\\n✅ 모델 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 모델 불러오기 및 테스트\n",
    "############################################################################\n",
    "\n",
    "print(\"📂 모델 불러오기 및 테스트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. JSON 설정에서 불러오기\n",
    "print(\"1. JSON 설정에서 모델 불러오기\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    json_components, json_metadata = model_manager.load_model_config(model_name)\n",
    "    print(f\"✅ JSON 불러오기 성공!\")\n",
    "    print(f\"📊 불러온 컴포넌트: {list(json_components.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ JSON 불러오기 실패: {e}\")\n",
    "    json_components = None\n",
    "\n",
    "# 2. Pickle에서 불러오기\n",
    "print(\"\\n2. Pickle에서 모델 불러오기\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    pickle_components, pickle_metadata = model_manager.load_model_pickle(model_name)\n",
    "    print(f\"✅ Pickle 불러오기 성공!\")\n",
    "    print(f\"📊 불러온 컴포넌트: {list(pickle_components.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pickle 불러오기 실패: {e}\")\n",
    "    pickle_components = None\n",
    "\n",
    "# 3. 불러온 모델들 성능 테스트\n",
    "print(\"\\n3. 불러온 모델들 성능 테스트\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_question = \"LangChain 모델 직렬화의 장점은 무엇인가요?\"\n",
    "print(f\"🧪 테스트 질문: {test_question}\")\n",
    "\n",
    "# JSON 모델 테스트\n",
    "if json_components and \"prompt\" in json_components:\n",
    "    try:\n",
    "        json_prompt = json_components[\"prompt\"]\n",
    "        json_messages = json_prompt.format_messages(question=test_question)\n",
    "        json_response = llm.invoke(json_messages)\n",
    "        print(f\"\\n📄 JSON 모델 응답: {json_response.content[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ JSON 모델 테스트 실패: {e}\")\n",
    "\n",
    "# Pickle 모델 테스트\n",
    "if pickle_components:\n",
    "    try:\n",
    "        pickle_chain = pickle_components[\"chain\"]\n",
    "        pickle_response = pickle_chain.invoke({\"question\": test_question})\n",
    "        print(f\"\\n🥒 Pickle 모델 응답: {pickle_response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pickle 모델 테스트 실패: {e}\")\n",
    "\n",
    "print(\"\\n✅ 모델 불러오기 및 테스트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843fc00",
   "metadata": {},
   "source": [
    "## 6. 체인 직렬화 실습\n",
    "\n",
    "복잡한 LangChain 체인의 직렬화를 연습해봅시다.\n",
    "\n",
    "### 체인 직렬화의 중요성\n",
    "- **재사용성**: 복잡한 체인 구성을 재사용\n",
    "- **배포**: 프로덕션 환경에 체인 배포\n",
    "- **버전 관리**: 체인의 다양한 버전 관리\n",
    "- **협업**: 팀원 간 체인 공유\n",
    "\n",
    "### 직렬화 가능한 체인 vs 불가능한 체인\n",
    "- ✅ **가능**: LangChain 기본 컴포넌트로만 구성된 체인\n",
    "- ❌ **불가능**: 사용자 정의 함수나 람다가 포함된 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 복잡한 체인 생성 및 직렬화\n",
    "# 💡LangChain에서 직렬화 가능한 체인을 만들 때의 핵심 원칙:\n",
    "#   - 가능한 한 LangChain의 기본 컴포넌트만 사용\n",
    "#   - 커스텀 함수나 람다 함수는 직렬화되지 않으므로 피하기\n",
    "#   - 프롬프트 템플릿에 고정 값들을 직접 포함시키기\n",
    "############################################################################\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "print(\"🔗 복잡한 체인 직렬화 실습\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 다양한 타입의 체인 생성\n",
    "print(\"1. 다양한 타입의 체인 생성\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 기본 순차 체인\n",
    "basic_chain = prompt | llm | parser\n",
    "print(f\"✅ 기본 체인: {is_lc_serializable(basic_chain)}\")\n",
    "\n",
    "# 병렬 체인\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"다음 텍스트를 요약해주세요: {text}\")\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"다음 텍스트의 주요 키워드를 추출해주세요: {text}\")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_prompt | llm | parser,\n",
    "    \"keywords\": analysis_prompt | llm | parser\n",
    "})\n",
    "print(f\"✅ 병렬 체인: {is_lc_serializable(parallel_chain)}\")\n",
    "\n",
    "# 조건부 체인 (더 간단한 접근법)\n",
    "conditional_prompt = ChatPromptTemplate.from_template(\n",
    "    \"질문: {question}\\n\\n답변해주세요:\"\n",
    ")\n",
    "\n",
    "# 단순한 체인으로 구성 (직렬화 가능)\n",
    "conditional_chain = conditional_prompt | llm | parser\n",
    "print(f\"✅ 조건부 체인: {is_lc_serializable(conditional_chain)}\")\n",
    "\n",
    "# 또는 질문 타입을 포함한 다른 접근법\n",
    "enhanced_prompt = ChatPromptTemplate.from_template(\n",
    "    \"다음은 일반적인 질문입니다.\\n질문: {question}\\n\\n답변해주세요:\"\n",
    ")\n",
    "\n",
    "enhanced_chain = enhanced_prompt | llm | parser\n",
    "print(f\"✅ 향상된 체인: {is_lc_serializable(enhanced_chain)}\")\n",
    "\n",
    "# 2. 체인들의 직렬화 테스트\n",
    "print(\"\\n2. 체인들의 직렬화 테스트\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "chains_to_test = {\n",
    "    \"basic_chain\": basic_chain,\n",
    "    \"parallel_chain\": parallel_chain,\n",
    "    \"conditional_chain\": conditional_chain,\n",
    "    \"enhanced_chain\": enhanced_chain\n",
    "}\n",
    "\n",
    "serialized_chains = {}\n",
    "\n",
    "for name, chain in chains_to_test.items():\n",
    "    try:\n",
    "        # JSON 직렬화 시도\n",
    "        chain_json = dumps(chain)\n",
    "        serialized_chains[name] = chain_json\n",
    "        print(f\"✅ {name}: JSON 직렬화 성공 ({len(chain_json)} 문자)\")\n",
    "        \n",
    "        # 역직렬화 테스트\n",
    "        restored_chain = loads(chain_json)\n",
    "        print(f\"   ↳ 역직렬화: ✅ 성공\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: 직렬화 실패 - {e}\")\n",
    "\n",
    "print(\"\\n✅ 체인 직렬화 테스트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5322c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 직렬화된 체인 실행 테스트\n",
    "############################################################################\n",
    "\n",
    "print(\"🧪 직렬화된 체인 실행 테스트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 테스트 데이터\n",
    "test_inputs = {\n",
    "    \"basic_chain\": {\"question\": \"LangChain이란 무엇인가요?\"},\n",
    "    \"parallel_chain\": {\"text\": \"LangChain은 대규모 언어 모델을 활용한 애플리케이션 개발 프레임워크입니다.\"},\n",
    "    \"conditional_chain\": {\"question\": \"Python의 장점은 무엇인가요?\"},\n",
    "    \"enhanced_chain\": {\"question\": \"AI의 미래는 어떨까요?\"}\n",
    "}\n",
    "\n",
    "# 각 체인 실행 테스트\n",
    "for chain_name in serialized_chains.keys():\n",
    "    if chain_name in test_inputs:\n",
    "        print(f\"\\n🔗 {chain_name} 테스트\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # 원본 체인 실행\n",
    "            original_chain = chains_to_test[chain_name]\n",
    "            original_result = original_chain.invoke(test_inputs[chain_name])\n",
    "            \n",
    "            # 직렬화된 체인 복원 및 실행\n",
    "            restored_chain = loads(serialized_chains[chain_name])\n",
    "            restored_result = restored_chain.invoke(test_inputs[chain_name])\n",
    "            \n",
    "            print(f\"📤 입력: {test_inputs[chain_name]}\")\n",
    "            \n",
    "            if isinstance(original_result, dict):\n",
    "                print(f\"📥 원본 결과: {str(original_result)[:100]}...\")\n",
    "                print(f\"🔄 복원 결과: {str(restored_result)[:100]}...\")\n",
    "            else:\n",
    "                print(f\"📥 원본 결과: {original_result[:100]}...\")\n",
    "                print(f\"🔄 복원 결과: {restored_result[:100]}...\")\n",
    "                \n",
    "            print(\"✅ 실행 성공!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실행 실패: {e}\")\n",
    "\n",
    "print(\"\\n\\n🎯 체인 직렬화 실습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd059ec",
   "metadata": {},
   "source": [
    "## 정리 및 모범 사례\n",
    "\n",
    "이 노트북에서 배운 LangChain 모델 직렬화 방법들을 정리해봅시다.\n",
    "\n",
    "### 🎯 주요 학습 내용\n",
    "\n",
    "1. **직렬화 가능성 확인**: `is_lc_serializable()` 함수 활용\n",
    "2. **JSON 직렬화**: `dumps()`, `dumpd()`, `loads()` 함수 사용\n",
    "3. **Pickle 직렬화**: Python의 `pickle` 모듈 활용\n",
    "4. **모델 관리**: 체계적인 저장/불러오기 시스템 구축\n",
    "5. **체인 직렬화**: 복잡한 체인의 저장과 복원\n",
    "\n",
    "### 📋 직렬화 방법 선택 가이드\n",
    "\n",
    "| 상황 | 추천 방법 | 이유 |\n",
    "|------|-----------|------|\n",
    "| **설정 공유** | JSON (`dumps`) | 가독성, 플랫폼 독립성 |\n",
    "| **프로덕션 배포** | JSON + 검증 | 안정성, 버전 관리 |\n",
    "| **개발/테스트** | Pickle | 완전성, 편의성 |\n",
    "| **팀 협업** | JSON | 호환성, 디버깅 용이 |\n",
    "| **모델 백업** | Pickle | 전체 상태 보존 |\n",
    "\n",
    "### ⚠️ 주의사항\n",
    "\n",
    "1. **보안**: Pickle 파일은 신뢰할 수 있는 소스에서만 로드\n",
    "2. **버전 호환성**: LangChain 버전 변경 시 직렬화 호환성 확인\n",
    "3. **API 키**: 직렬화 시 민감한 정보 제외\n",
    "4. **파일 관리**: 정기적인 정리로 디스크 공간 관리\n",
    "5. **테스트**: 직렬화/역직렬화 후 항상 동작 검증\n",
    "\n",
    "### 🚀 실전 활용 팁\n",
    "\n",
    "1. **버전 태깅**: 모델 파일에 버전 정보 포함\n",
    "2. **메타데이터**: 생성 일시, 환경 정보 저장\n",
    "3. **에러 처리**: 로딩 실패 시 대체 방안 준비\n",
    "4. **성능 모니터링**: 직렬화된 모델의 성능 추적\n",
    "5. **백업 전략**: 중요한 모델의 다중 백업 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9056a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# 생성된 파일들 정리 (선택사항)\n",
    "############################################################################\n",
    "\n",
    "print(\"🧹 생성된 파일들 정리\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 정리할 파일들 목록\n",
    "files_to_clean = [\n",
    "    \"prompt_template.pkl\",\n",
    "    \"model_components.pkl\"\n",
    "]\n",
    "\n",
    "# 생성된 모델 디렉토리도 포함\n",
    "if model_manager.base_dir.exists():\n",
    "    for file in model_manager.base_dir.glob(\"*\"):\n",
    "        files_to_clean.append(str(file))\n",
    "\n",
    "print(\"📁 정리 대상 파일들:\")\n",
    "for file_path in files_to_clean:\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"   {file_path} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"   {file_path} (존재하지 않음)\")\n",
    "\n",
    "# 파일 삭제 여부 선택\n",
    "print(f\"\\n총 {len([f for f in files_to_clean if os.path.exists(f)])} 개의 파일이 정리 대상입니다.\")\n",
    "print(\"파일들을 삭제하려면 아래 줄의 주석을 해제하세요:\")\n",
    "print(\"# cleanup_files = True\")\n",
    "\n",
    "cleanup_files = False  # True로 변경하면 파일 삭제\n",
    "\n",
    "if cleanup_files:\n",
    "    deleted_count = 0\n",
    "    for file_path in files_to_clean:\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"🗑️  삭제됨: {file_path}\")\n",
    "                    deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 삭제 실패: {file_path} - {e}\")\n",
    "    \n",
    "    # 빈 디렉토리 삭제\n",
    "    try:\n",
    "        if model_manager.base_dir.exists() and not any(model_manager.base_dir.iterdir()):\n",
    "            model_manager.base_dir.rmdir()\n",
    "            print(f\"🗑️  빈 디렉토리 삭제: {model_manager.base_dir}\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    print(f\"\\n✅ 총 {deleted_count}개 파일이 정리되었습니다.\")\n",
    "else:\n",
    "    print(\"\\n💡 정리를 원하면 위의 cleanup_files 변수를 True로 설정하세요.\")\n",
    "\n",
    "print(\"\\n🎉 LangChain 모델 직렬화 실습이 완료되었습니다!\")\n",
    "print(\"📚 배운 내용을 실제 프로젝트에 적용해보세요!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
